{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前设备名称： NVIDIA GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"当前设备名称：\", torch.cuda.get_device_name(device) if device.type == 'cuda' else 'CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 生成词索引构成的序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4, 6, 0, 0, 0],\n",
      "        [2, 5, 3, 3, 0]])\n",
      "tensor([[1, 7, 1, 3, 0],\n",
      "        [1, 5, 1, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "max_num_src_words = 8\n",
    "max_num_tgt_words = 8\n",
    "model_dim = 8\n",
    "max_src_seq_len = 5\n",
    "max_tgt_seq_len = 5\n",
    "max_position_len = 5\n",
    "# src_len = torch.randint(2,5,(batch_size,))\n",
    "# tgt_len = torch.randint(2,5,(batch_size,))\n",
    "src_len = torch.Tensor([2, 4]).to(torch.int32)\n",
    "tgt_len = torch.Tensor([4, 3]).to(torch.int32)\n",
    "# 词索引构成的序列\n",
    "src_seq = torch.cat([torch.unsqueeze(F.pad(torch.randint(1, max_num_src_words, (L,)), (0, max_src_seq_len-L)), 0) \\\n",
    "                     for L in src_len])\n",
    "tgt_seq = torch.cat([torch.unsqueeze(F.pad(torch.randint(1, max_num_tgt_words, (L,)), (0, max_tgt_seq_len-L)), 0) \\\n",
    "                     for L in tgt_len])\n",
    "print(src_seq)\n",
    "print(tgt_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 采用Embedding的forward方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.5966,  0.1909,  0.4394, -2.2050, -1.3832,  0.1112, -3.2345,\n",
      "          -2.2146],\n",
      "         [ 1.6877, -0.2476, -0.0546,  1.7473, -0.6367, -1.5957, -0.4812,\n",
      "          -0.0758],\n",
      "         [-0.7268, -1.8190,  1.0741, -0.7400,  0.9868, -0.5670,  0.5519,\n",
      "          -1.0576],\n",
      "         [-0.7268, -1.8190,  1.0741, -0.7400,  0.9868, -0.5670,  0.5519,\n",
      "          -1.0576],\n",
      "         [-0.7268, -1.8190,  1.0741, -0.7400,  0.9868, -0.5670,  0.5519,\n",
      "          -1.0576]],\n",
      "\n",
      "        [[ 0.9717,  0.1816, -2.0498, -1.1905, -0.9170, -1.3809, -0.9095,\n",
      "          -1.5783],\n",
      "         [ 0.6777,  1.3585,  0.7758,  2.8476,  1.2247, -1.1502,  0.9926,\n",
      "          -0.1122],\n",
      "         [-0.9992,  0.7478,  2.4502, -0.4837,  1.7873, -0.4702,  1.3554,\n",
      "          -0.1429],\n",
      "         [-0.9992,  0.7478,  2.4502, -0.4837,  1.7873, -0.4702,  1.3554,\n",
      "          -0.1429],\n",
      "         [-0.7268, -1.8190,  1.0741, -0.7400,  0.9868, -0.5670,  0.5519,\n",
      "          -1.0576]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 实例化embedding # requires_grad \n",
    "# weight是随机的，训练时更新\n",
    "src_embedding_table = nn.Embedding(max_num_src_words+1, model_dim)\n",
    "tgt_embedding_table = nn.Embedding(max_num_tgt_words+1, model_dim)\n",
    "# print(src_embedding_table.weight, tgt_embedding_table.weight)\n",
    "# Embedding存在 --call-- forward方法隐式\n",
    "src_embedding = src_embedding_table(src_seq)\n",
    "tgt_embedding_table = tgt_embedding_table(tgt_seq)\n",
    "print(src_embedding)\n",
    "# 由于为0 底下3排一样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 生成pos—embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np Time (microseconds): 1000.6427764892578\n",
      "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "          1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "        [ 8.4147e-01,  9.5042e-01,  9.9833e-02,  9.9950e-01,  9.9998e-03,\n",
      "          9.9999e-01,  1.0000e-03,  1.0000e+00],\n",
      "        [ 9.0930e-01,  8.0658e-01,  1.9867e-01,  9.9800e-01,  1.9999e-02,\n",
      "          9.9998e-01,  2.0000e-03,  1.0000e+00],\n",
      "        [ 1.4112e-01,  5.8275e-01,  2.9552e-01,  9.9550e-01,  2.9995e-02,\n",
      "          9.9995e-01,  3.0000e-03,  1.0000e+00],\n",
      "        [-7.5680e-01,  3.0114e-01,  3.8942e-01,  9.9201e-01,  3.9989e-02,\n",
      "          9.9992e-01,  4.0000e-03,  1.0000e+00]])\n",
      "torch Time (microseconds): 1998.9013671875\n",
      "tensor([[0, 1, 2, 3, 4],\n",
      "        [0, 1, 2, 3, 4]])\n",
      "tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "           1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "         [ 8.4147e-01,  9.5042e-01,  9.9833e-02,  9.9950e-01,  9.9998e-03,\n",
      "           9.9999e-01,  1.0000e-03,  1.0000e+00],\n",
      "         [ 9.0930e-01,  8.0658e-01,  1.9867e-01,  9.9800e-01,  1.9999e-02,\n",
      "           9.9998e-01,  2.0000e-03,  1.0000e+00],\n",
      "         [ 1.4112e-01,  5.8275e-01,  2.9552e-01,  9.9550e-01,  2.9995e-02,\n",
      "           9.9995e-01,  3.0000e-03,  1.0000e+00],\n",
      "         [-7.5680e-01,  3.0114e-01,  3.8942e-01,  9.9201e-01,  3.9989e-02,\n",
      "           9.9992e-01,  4.0000e-03,  1.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "           1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "         [ 8.4147e-01,  9.5042e-01,  9.9833e-02,  9.9950e-01,  9.9998e-03,\n",
      "           9.9999e-01,  1.0000e-03,  1.0000e+00],\n",
      "         [ 9.0930e-01,  8.0658e-01,  1.9867e-01,  9.9800e-01,  1.9999e-02,\n",
      "           9.9998e-01,  2.0000e-03,  1.0000e+00],\n",
      "         [ 1.4112e-01,  5.8275e-01,  2.9552e-01,  9.9550e-01,  2.9995e-02,\n",
      "           9.9995e-01,  3.0000e-03,  1.0000e+00],\n",
      "         [-7.5680e-01,  3.0114e-01,  3.8942e-01,  9.9201e-01,  3.9989e-02,\n",
      "           9.9992e-01,  4.0000e-03,  1.0000e+00]]])\n"
     ]
    }
   ],
   "source": [
    "# pos embedding    method1 \n",
    "start_time = time.time()\n",
    "pos_embedding = torch.zeros(max_position_len, model_dim)\n",
    "for i in range(0, model_dim):\n",
    "    for j in range(0, max_position_len):\n",
    "        if i%2 == 0:\n",
    "            # print(i,j)\n",
    "            pos_embedding[j, i] = np.sin(j/np.power(10000, (i/model_dim)))\n",
    "        else:\n",
    "            pos_embedding[j, i] = np.cos(j/np.power(10000, (i/model_dim)))\n",
    "# print(pos_embedding)\n",
    "end_time = time.time()\n",
    "# 计算执行时间并转换为微秒\n",
    "execution_time_microseconds = (end_time - start_time) * 1e6\n",
    "# 打印执行时间\n",
    "print(\"np Time (microseconds):\", execution_time_microseconds)\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////\n",
    "# pos embedding    method2 \n",
    "start_time = time.time()\n",
    "pos_mat = torch.arange(max_position_len).reshape(-1, 1) \n",
    "i_mat = torch.pow(10000, torch.arange(0, 8, 2).reshape((1, -1))/model_dim)\n",
    "i1_mat = torch.pow(10000, torch.arange(1, 8, 2).reshape((1, -1))/model_dim)\n",
    "# print(pos_mat, i_mat, i1_mat)\n",
    "pe_embedding_table = torch.zeros(max_position_len, model_dim)\n",
    "pe_embedding_table[:, 0::2] = torch.sin(pos_mat/i_mat)\n",
    "pe_embedding_table[:, 1::2] = torch.cos(pos_mat/i1_mat)\n",
    "print(pe_embedding_table)\n",
    "end_time = time.time()\n",
    "# 计算执行时间并转换为微秒\n",
    "execution_time_microseconds = (end_time - start_time) * 1e6\n",
    "# 打印执行时间\n",
    "print(\"torch Time (microseconds):\", execution_time_microseconds)\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////\n",
    "# print(torch.sub(pos_embedding, pe_embedding_table))\n",
    "pe_embedding = nn.Embedding(max_position_len, model_dim)\n",
    "# 位置编码训练中不发生变化requires_grad=False\n",
    "pe_embedding.weight = nn.Parameter(pe_embedding_table, requires_grad=False)\n",
    "# 注意 这里给embedding的是位置 不是词的编号\n",
    "# src_p=torch.zeros_like(src_seq)\n",
    "# for i in range(src_p.size(0)):\n",
    "#     for j in range(src_p.size(1)):\n",
    "#         src_p[i][j] = j \n",
    "# print(src_p)\n",
    "# src_pe_embedding =pe_embedding(src_p)\n",
    "src_pe_embedding =pe_embedding(torch.cat([pos_mat,pos_mat], 1).transpose(-1,-2))\n",
    "print(torch.cat([pos_mat,pos_mat], 1).transpose(-1,-2))\n",
    "print(src_pe_embedding)\n",
    "tgt_pe_embedding = src_pe_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 生成enc_self_attn_mask 无因果关系   对词向量相似度进行masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 1]) \n",
      " tensor([[[1.],\n",
      "         [1.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]]])\n",
      "tensor([[[False, False,  True,  True],\n",
      "         [False, False,  True,  True],\n",
      "         [ True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True]],\n",
      "\n",
      "        [[False, False, False, False],\n",
      "         [False, False, False, False],\n",
      "         [False, False, False, False],\n",
      "         [False, False, False, False]]])\n",
      "tensor([[[0.1765, 0.8235, 0.0000, 0.0000],\n",
      "         [0.5419, 0.4581, 0.0000, 0.0000],\n",
      "         [   nan,    nan,    nan,    nan],\n",
      "         [   nan,    nan,    nan,    nan]],\n",
      "\n",
      "        [[0.2057, 0.1508, 0.4783, 0.1653],\n",
      "         [0.0836, 0.7319, 0.0591, 0.1255],\n",
      "         [0.4394, 0.0688, 0.3904, 0.1014],\n",
      "         [0.0811, 0.6896, 0.0911, 0.1381]]])\n"
     ]
    }
   ],
   "source": [
    "# 这里不是三角矩阵# 这里不是三角矩阵 没有因果关系 多用矩阵相乘 转置 升维\n",
    "vaild_encoder_pos = torch.unsqueeze(torch.cat([torch.unsqueeze(F.pad(torch.ones(L),(0, max(src_len)-L)),0) for L in src_len]), 2)\n",
    "# print(vaild_encoder_pos.shape, \"\\n\", vaild_encoder_pos)\n",
    "# vaild_encoder_pos = torch.cat([torch.unsqueeze(F.pad(torch.ones(L),(0, max(src_len)-L)),0) for L in src_len])\n",
    "print(vaild_encoder_pos.shape, \"\\n\", vaild_encoder_pos)\n",
    "# 这里不是三角矩阵\n",
    "# print(torch.tril(torch.ones(4, 4)))\n",
    "# vaild_mask = vaild_encoder_pos.transpose(1,2)*torch.ones(4, 4)*vaild_encoder_pos\n",
    "# print(vaild_mask.shape, \"\\n\", vaild_mask)\n",
    "# print(torch.bmm(torch.unsqueeze(torch.ones(4, 4),0), torch.unsqueeze(torch.ones(4, 4),0)))\n",
    "# print(torch.ones(4, 4)*torch.ones(4, 4))\n",
    "valid_encoder_pos_matrix = torch.bmm(vaild_encoder_pos, vaild_encoder_pos.transpose(1, 2))\n",
    "invalid_encoder_pos_matrix = 1-valid_encoder_pos_matrix\n",
    "mask_encoder_self_attention = invalid_encoder_pos_matrix.to(torch.bool)\n",
    "print(mask_encoder_self_attention)\n",
    "#假设是vk注意力权重\n",
    "score = torch.randn(batch_size, max(src_len), max(src_len))\n",
    "masked_score = score.masked_fill(mask_encoder_self_attention, -np.inf)\n",
    "prob = F.softmax(masked_score, -1)\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> get_attn_pad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 1]) \n",
      " torch.Size([2, 1, 4])\n",
      "tensor([[[1.],\n",
      "         [1.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]]]) \n",
      " tensor([[[1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 0.]]])\n",
      "tensor([[[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1., 0.],\n",
      "         [1., 1., 1., 0.],\n",
      "         [1., 1., 1., 0.],\n",
      "         [1., 1., 1., 0.]]]) \n",
      " torch.Size([2, 4, 4])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "masked_fill_ only supports boolean masks, but got mask with dtype float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[169], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(get_attn_pad_mask, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, get_attn_pad_mask\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      8\u001b[0m score \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;28mmax\u001b[39m(src_len), \u001b[38;5;28mmax\u001b[39m(tgt_len))\n\u001b[1;32m----> 9\u001b[0m masked_score \u001b[38;5;241m=\u001b[39m \u001b[43mscore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_attn_pad_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m prob \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(masked_score\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mbool), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(prob)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: masked_fill_ only supports boolean masks, but got mask with dtype float"
     ]
    }
   ],
   "source": [
    "enc_encoder_pos = torch.unsqueeze(torch.cat([torch.unsqueeze(F.pad(torch.ones(L),(0, max(src_len)-L)),0) for L in src_len]), 2)\n",
    "dec_encoder_pos = torch.unsqueeze(torch.cat([torch.unsqueeze(F.pad(torch.ones(L),(0, max(tgt_len)-L)),0) for L in tgt_len]), 1)\n",
    "print(enc_encoder_pos.shape, \"\\n\", dec_encoder_pos.shape)\n",
    "print(enc_encoder_pos, \"\\n\", dec_encoder_pos)\n",
    "# 具体维度通过【】的奇偶特征决定\n",
    "get_attn_pad_mask = torch.bmm(enc_encoder_pos, dec_encoder_pos)\n",
    "print(get_attn_pad_mask, \"\\n\", get_attn_pad_mask.shape)\n",
    "score = torch.randn(2, max(src_len), max(tgt_len))\n",
    "masked_score = score.masked_fill(get_attn_pad_mask, -np.inf)\n",
    "prob = F.softmax((1-masked_score).to(torch.bool), -1)\n",
    "print(prob)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
