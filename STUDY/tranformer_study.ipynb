{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前设备名称： NVIDIA GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"当前设备名称：\", torch.cuda.get_device_name(device) if device.type == 'cuda' else 'CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 生成词索引构成的序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7, 4, 0, 0, 0],\n",
      "        [7, 2, 5, 4, 0]])\n",
      "tensor([[2, 3, 6, 5, 0],\n",
      "        [4, 5, 5, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "max_num_src_words = 8\n",
    "max_num_tgt_words = 8\n",
    "model_dim = 8\n",
    "max_src_seq_len = 5\n",
    "max_tgt_seq_len = 5\n",
    "max_position_len = 5\n",
    "# src_len = torch.randint(2,5,(batch_size,))\n",
    "# tgt_len = torch.randint(2,5,(batch_size,))\n",
    "src_len = torch.Tensor([2, 4]).to(torch.int32)\n",
    "tgt_len = torch.Tensor([4, 3]).to(torch.int32)\n",
    "# 词索引构成的序列\n",
    "src_seq = torch.cat([torch.unsqueeze(F.pad(torch.randint(1, max_num_src_words, (L,)), (0, max_src_seq_len-L)), 0) \\\n",
    "                     for L in src_len])\n",
    "tgt_seq = torch.cat([torch.unsqueeze(F.pad(torch.randint(1, max_num_tgt_words, (L,)), (0, max_tgt_seq_len-L)), 0) \\\n",
    "                     for L in tgt_len])\n",
    "print(src_seq)\n",
    "print(tgt_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 采用Embedding的forward方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3390,  0.2224,  1.7262, -2.0615, -1.6018,  0.4065,  0.5479,\n",
      "           0.2578],\n",
      "         [ 0.7082,  0.0977,  0.6959,  1.0974, -0.7529,  0.6294,  0.7810,\n",
      "          -0.6112],\n",
      "         [-0.8659,  0.3368, -0.1845, -1.0268, -0.1665, -0.6293, -0.5064,\n",
      "           1.3912],\n",
      "         [-0.8659,  0.3368, -0.1845, -1.0268, -0.1665, -0.6293, -0.5064,\n",
      "           1.3912],\n",
      "         [-0.8659,  0.3368, -0.1845, -1.0268, -0.1665, -0.6293, -0.5064,\n",
      "           1.3912]],\n",
      "\n",
      "        [[ 0.3390,  0.2224,  1.7262, -2.0615, -1.6018,  0.4065,  0.5479,\n",
      "           0.2578],\n",
      "         [ 2.6824,  0.9109, -0.0814,  0.9007, -0.3474,  2.5647, -0.4796,\n",
      "          -1.1515],\n",
      "         [-1.3354,  1.2109, -1.0373, -0.4644, -0.4364, -0.0551, -1.2579,\n",
      "           0.2740],\n",
      "         [ 0.7082,  0.0977,  0.6959,  1.0974, -0.7529,  0.6294,  0.7810,\n",
      "          -0.6112],\n",
      "         [-0.8659,  0.3368, -0.1845, -1.0268, -0.1665, -0.6293, -0.5064,\n",
      "           1.3912]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 实例化embedding # requires_grad \n",
    "# weight是随机的，训练时更新\n",
    "src_embedding_table = nn.Embedding(max_num_src_words+1, model_dim)\n",
    "tgt_embedding_table = nn.Embedding(max_num_tgt_words+1, model_dim)\n",
    "# print(src_embedding_table.weight, tgt_embedding_table.weight)\n",
    "# Embedding存在 --call-- forward方法隐式\n",
    "src_embedding = src_embedding_table(src_seq)\n",
    "tgt_embedding_table = tgt_embedding_table(tgt_seq)\n",
    "print(src_embedding)\n",
    "# 由于为0 底下3排一样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 生成pos—embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np Time (microseconds): 1003.9806365966797\n",
      "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "          1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "        [ 8.4147e-01,  9.5042e-01,  9.9833e-02,  9.9950e-01,  9.9998e-03,\n",
      "          9.9999e-01,  1.0000e-03,  1.0000e+00],\n",
      "        [ 9.0930e-01,  8.0658e-01,  1.9867e-01,  9.9800e-01,  1.9999e-02,\n",
      "          9.9998e-01,  2.0000e-03,  1.0000e+00],\n",
      "        [ 1.4112e-01,  5.8275e-01,  2.9552e-01,  9.9550e-01,  2.9995e-02,\n",
      "          9.9995e-01,  3.0000e-03,  1.0000e+00],\n",
      "        [-7.5680e-01,  3.0114e-01,  3.8942e-01,  9.9201e-01,  3.9989e-02,\n",
      "          9.9992e-01,  4.0000e-03,  1.0000e+00]])\n",
      "torch Time (microseconds): 4999.399185180664\n",
      "tensor([[0, 1, 2, 3, 4],\n",
      "        [0, 1, 2, 3, 4]])\n",
      "tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "           1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "         [ 8.4147e-01,  9.5042e-01,  9.9833e-02,  9.9950e-01,  9.9998e-03,\n",
      "           9.9999e-01,  1.0000e-03,  1.0000e+00],\n",
      "         [ 9.0930e-01,  8.0658e-01,  1.9867e-01,  9.9800e-01,  1.9999e-02,\n",
      "           9.9998e-01,  2.0000e-03,  1.0000e+00],\n",
      "         [ 1.4112e-01,  5.8275e-01,  2.9552e-01,  9.9550e-01,  2.9995e-02,\n",
      "           9.9995e-01,  3.0000e-03,  1.0000e+00],\n",
      "         [-7.5680e-01,  3.0114e-01,  3.8942e-01,  9.9201e-01,  3.9989e-02,\n",
      "           9.9992e-01,  4.0000e-03,  1.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "           1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "         [ 8.4147e-01,  9.5042e-01,  9.9833e-02,  9.9950e-01,  9.9998e-03,\n",
      "           9.9999e-01,  1.0000e-03,  1.0000e+00],\n",
      "         [ 9.0930e-01,  8.0658e-01,  1.9867e-01,  9.9800e-01,  1.9999e-02,\n",
      "           9.9998e-01,  2.0000e-03,  1.0000e+00],\n",
      "         [ 1.4112e-01,  5.8275e-01,  2.9552e-01,  9.9550e-01,  2.9995e-02,\n",
      "           9.9995e-01,  3.0000e-03,  1.0000e+00],\n",
      "         [-7.5680e-01,  3.0114e-01,  3.8942e-01,  9.9201e-01,  3.9989e-02,\n",
      "           9.9992e-01,  4.0000e-03,  1.0000e+00]]])\n"
     ]
    }
   ],
   "source": [
    "# pos embedding    method1 \n",
    "start_time = time.time()\n",
    "pos_embedding = torch.zeros(max_position_len, model_dim)\n",
    "for i in range(0, model_dim):\n",
    "    for j in range(0, max_position_len):\n",
    "        if i%2 == 0:\n",
    "            # print(i,j)\n",
    "            pos_embedding[j, i] = np.sin(j/np.power(10000, (i/model_dim)))\n",
    "        else:\n",
    "            pos_embedding[j, i] = np.cos(j/np.power(10000, (i/model_dim)))\n",
    "# print(pos_embedding)\n",
    "end_time = time.time()\n",
    "# 计算执行时间并转换为微秒\n",
    "execution_time_microseconds = (end_time - start_time) * 1e6\n",
    "# 打印执行时间\n",
    "print(\"np Time (microseconds):\", execution_time_microseconds)\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////\n",
    "# pos embedding    method2 \n",
    "start_time = time.time()\n",
    "pos_mat = torch.arange(max_position_len).reshape(-1, 1) \n",
    "i_mat = torch.pow(10000, torch.arange(0, 8, 2).reshape((1, -1))/model_dim)\n",
    "i1_mat = torch.pow(10000, torch.arange(1, 8, 2).reshape((1, -1))/model_dim)\n",
    "# print(pos_mat, i_mat, i1_mat)\n",
    "pe_embedding_table = torch.zeros(max_position_len, model_dim)\n",
    "pe_embedding_table[:, 0::2] = torch.sin(pos_mat/i_mat)\n",
    "pe_embedding_table[:, 1::2] = torch.cos(pos_mat/i1_mat)\n",
    "print(pe_embedding_table)\n",
    "end_time = time.time()\n",
    "# 计算执行时间并转换为微秒\n",
    "execution_time_microseconds = (end_time - start_time) * 1e6\n",
    "# 打印执行时间\n",
    "print(\"torch Time (microseconds):\", execution_time_microseconds)\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////\n",
    "# print(torch.sub(pos_embedding, pe_embedding_table))\n",
    "pe_embedding = nn.Embedding(max_position_len, model_dim)\n",
    "# 位置编码训练中不发生变化requires_grad=False\n",
    "pe_embedding.weight = nn.Parameter(pe_embedding_table, requires_grad=False)\n",
    "# 注意 这里给embedding的是位置 不是词的编号\n",
    "# src_p=torch.zeros_like(src_seq)\n",
    "# for i in range(src_p.size(0)):\n",
    "#     for j in range(src_p.size(1)):\n",
    "#         src_p[i][j] = j \n",
    "# print(src_p)\n",
    "# src_pe_embedding =pe_embedding(src_p)\n",
    "src_pe_embedding =pe_embedding(torch.cat([pos_mat,pos_mat], 1).transpose(-1,-2))\n",
    "print(torch.cat([pos_mat,pos_mat], 1).transpose(-1,-2))\n",
    "print(src_pe_embedding)\n",
    "tgt_pe_embedding = src_pe_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 生成enc_self_attn_mask 无因果关系   对词向量相似度进行masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 1]) \n",
      " tensor([[[1.],\n",
      "         [1.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]]])\n",
      "tensor([[[False, False,  True,  True],\n",
      "         [False, False,  True,  True],\n",
      "         [ True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True]],\n",
      "\n",
      "        [[False, False, False, False],\n",
      "         [False, False, False, False],\n",
      "         [False, False, False, False],\n",
      "         [False, False, False, False]]])\n",
      "tensor([[[0.0295, 0.9705, 0.0000, 0.0000],\n",
      "         [0.3952, 0.6048, 0.0000, 0.0000],\n",
      "         [   nan,    nan,    nan,    nan],\n",
      "         [   nan,    nan,    nan,    nan]],\n",
      "\n",
      "        [[0.1989, 0.2732, 0.3632, 0.1647],\n",
      "         [0.6721, 0.0277, 0.2126, 0.0876],\n",
      "         [0.4523, 0.1101, 0.1889, 0.2487],\n",
      "         [0.1467, 0.1221, 0.4143, 0.3169]]])\n"
     ]
    }
   ],
   "source": [
    "# 这里不是三角矩阵# 这里不是三角矩阵 没有因果关系 多用矩阵相乘 转置 升维\n",
    "vaild_encoder_pos = torch.unsqueeze(torch.cat([torch.unsqueeze(F.pad(torch.ones(L),(0, max(src_len)-L)),0) for L in src_len]), 2)\n",
    "# print(vaild_encoder_pos.shape, \"\\n\", vaild_encoder_pos)\n",
    "# vaild_encoder_pos = torch.cat([torch.unsqueeze(F.pad(torch.ones(L),(0, max(src_len)-L)),0) for L in src_len])\n",
    "print(vaild_encoder_pos.shape, \"\\n\", vaild_encoder_pos)\n",
    "# 这里不是三角矩阵\n",
    "# print(torch.tril(torch.ones(4, 4)))\n",
    "# vaild_mask = vaild_encoder_pos.transpose(1,2)*torch.ones(4, 4)*vaild_encoder_pos\n",
    "# print(vaild_mask.shape, \"\\n\", vaild_mask)\n",
    "# print(torch.bmm(torch.unsqueeze(torch.ones(4, 4),0), torch.unsqueeze(torch.ones(4, 4),0)))\n",
    "# print(torch.ones(4, 4)*torch.ones(4, 4))\n",
    "valid_encoder_pos_matrix = torch.bmm(vaild_encoder_pos, vaild_encoder_pos.transpose(1, 2))\n",
    "invalid_encoder_pos_matrix = 1-valid_encoder_pos_matrix\n",
    "mask_encoder_self_attention = invalid_encoder_pos_matrix.to(torch.bool)\n",
    "print(mask_encoder_self_attention)\n",
    "#假设是vk注意力权重\n",
    "score = torch.randn(batch_size, max(src_len), max(src_len))\n",
    "masked_score = score.masked_fill(mask_encoder_self_attention, -np.inf)\n",
    "prob = F.softmax(masked_score, -1)\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
